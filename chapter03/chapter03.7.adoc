== 3.7 The binomial distribution

Although somewhat complicated mathematically, the hypergeometric distribution arises from a problem that is very clear and simple conceptually; there are only a finite number of possibilities and all the above results are exact for the problems as stated. As an introduction to a mathematically simpler, but conceptually far more difficult, problem, we examine a limiting form of the hypergeometric distribution.

The complication of the hypergeometric distribution arises because it is taking into account the changing contents of the urn; knowing the result of any draw changes the probability for red for any other draw. But if the number N of balls in the urn is very large compared with the number drawn (N >> n), then this probability changes very little, and in the limit N →∞ we should have a simpler result, free of such dependencies. To verify this, we write the hypergeometric distribution (3.22) as

 $$h(r|N,M,n) = \frac {[ \frac{1}{N^r} \binom {M}{r}] [ \frac {1}{N^{n-r}} ( \binom {N-M}{n-r} ) ] } { \frac {1}{N^n} \binom {N}{n} ] } $$. (3.81)

The first factor is

 $$\frac {1}{N^r} \binom {M} {r} = \frac {1}{r!} \frac {M}{N} (\frac M N − \frac 1 N) (\frac M N − \frac 2 N) ··· ( \frac M N − \frac {r−1}{N}) $$ , (3.82)

and in the limit N →∞, M →∞, M/N → f , we have

 $$\frac {1}{N_r} \binom {M}{r} → \frac {f^r} {r!} $$. (3.83)

Likewise,

 $$\frac {1}{N^{n−r}} \binom {M−1}{n−r} → \frac {(1−f)^{n−r}} {(n−r)!} $$, (3.84)
 $$\frac {1}{N^n} \binom {N}{n} → \frac {1}{n!} $$. (3.85)

In principle, we should, of course, take the limit of the product in (3.81), not the product of the limits. But in (3.81) we have defined the factors so that each has its own independent limit, so the result is the same; the hypergeometric distribution goes into

 $$h(r|N,M,n) → b(r|n,f) ≡ \binom {n}{r} f^r (1 − f )^{n−r}$$ (3.86)

called the binomial distribution, because evaluation of the generating function (3.24) now reduces to

 $$G(t) ≡ \sum_{r=0}^n b(r|n,f)t^r = (1 − f + ft)^n, $$   (3.87)

an example of Newton’s binomial theorem.

Figure 3.1 compares three hypergeometric distributions with N = 15, 30, 100 and M/N = 0.4, n = 10 to the binomial distribution with n = 10, f = 0.4. All have their peak at r = 4, and all distributions have the same first moment $$\langle r \rangle$$ = E(r ) = 4, but the binomial distribution is broader.

The N = 15 hypergeometric distribution is zero for r = 0 and r > 6, since on drawing ten balls from an urn containing only six red and nine white, it is not possible to get fewer than one or more than six red balls. When N > 100 the hypergeometric distribution agrees so closely with the binomial that for most purposes it would not matter which one we used. Analytical properties of the binomial distribution are collected in Chapter 7. In Chapter 9 we find, in connection with significance tests, situations where the binomial distribution is exact for purely combinatorial reasons in a finite sample space, Eq. (9.46).

We can carry out a similar limiting process on the generalized hypergeometric distribution (3.75). It is left as an exercise to show that in the limit where all $$N_i →∞$$in such a way that the fractions

 $$f_i ≡ \frac {N_i}{\sum N_j} $$(3.88)

tend to constants, (3.75) goes into the multinomial distribution

 $$m(r_1···r_k|f_1 ··· f_k ) = \frac {r!} {r_1! ··· r_k!} f_1^{r1} ··· f_k^{rk}$$ , (3.89)

where $$r ≡ \sum ri$$ . And, as in (3.87), we can define a generating function of (k − 1) variables, from which we can prove that (3.89) is correctly normalized and derive many other useful results.

Exercise 3.2. Suppose an urn contains $$N = \sum N_i$$ balls, $$N_1$$ of color 1, $$N_2$$ of color 2, . . . , $$N_k$$ of color k. We draw m balls without replacement; what is the probability that we have at least one of each color? Supposing k = 5, all Ni = 10, how many do we need to draw in order to have at least a 90% probability for getting a full set?

Exercise 3.3. Suppose that in the previous exercise k is initially unknown, but we know that the urn contains exactly 50 balls. Drawing out 20 of them, we find three different colors; now what do we know about k? We know from deductive reasoning (i.e. with certainty) that 3 ≤ k ≤ 33; but can you set narrower limits $$k_1 ≤ k ≤ k_2$$ within which it is highly likely to be?

Hint: This question goes beyond the sampling theory of this chapter because, like most real scientific problems, the answer depends to some degree on our common sense judgments; nevertheless, our rules of probability theory are quite capable of dealing with it, and persons with reasonable common sense cannot differ appreciably in their conclusions.

Exercise 3.4. The M urns are now numbered 1 to M, and M balls, also numbered  1 to M, are thrown into them, one in each urn. If the numbers of a ball and its urn are the same, we have a match. Show that the probability for at least one match is

 $$h = \sum_{k=1}{M} (−1)^{k+1}/k!$$ (3.90)

As M →∞, this converges to 1 − 1/e = 0.632. The result is surprising to many, because, however large M is, there remains an appreciable probability for no match at all.

Exercise 3.5. N balls are tossed into M urns; there are evidently $$M_N$$ ways this can be done. If the robot considers them all equally likely, what is the probability that each urn receives at least one ball?
