== 4.4 Multiple hypothesis testing

Suppose that something very remarkable happens in the sequential test just discussed: we tested 50 widgets and every one turned out to be bad. According to (4.22), that would give us 150 db of evidence for the proposition that we had the bad batch. e(A|E) would end up at +140 db, which is a probability which differs from unity by one part in $$10^{14}$$. Now, our common sense rejects this conclusion; some kind of innate skepticism rises in us. If you test 50 widgets and you find that all 50 are bad, you are not willing to believe that you have a batch in which only one in three are really bad. So what went wrong here? Why doesn’t our robot work in this case?

We have to recognize that our robot is immature; it reasons like a four-year-old child does. The remarkable thing about small children is that you can tell them the most ridiculous things and they will accept it all with wide open eyes, open mouth, and it never occurs to them to question you. They will believe anything you tell them.

Adults learn to make mental allowance for the reliability of the source when told something hard to believe. One might think that, ideally, the information which our robot should have put into its memory was not that we had either 1/3 bad or 1/6 bad; the information it should have put in was that some unreliable human said that we had either 1/3 bad or 1/6 bad.

More generally, it might be useful in many problems if the robot could take into account the fact that the information it has been given may not be perfectly reliable to begin with. There is always a small chance that the prior information or data that we fed to the robot was wrong. In a real problem there are always hundreds of possibilities, and if you start out the robot with dogmatic initial statements which say that there are only two possibilities, then of course you must not expect its conclusions to make sense in every case.

To accomplish this skeptically mature behavior automatically in a robot is something that we can do, when we come to consider significance tests; but fortunately, after further reflection, we realize that for most problems the present immature robot is what we want after all, because we have better control over it.

We do want the robot to believe whatever we tell it; it would be dangerous to have a robot who suddenly became skeptical in a way not under our control when we tried to tell it some true but startling – and therefore highly important – new fact. But then the onus is on us to be aware of this situation, and when there is a good chance that skepticism will be needed, it is up to us to give the robot a hint about how to be skeptical for that particular problem.

In the present problem we can give the hint which makes the robot skeptical about A when it sees ‘too many’ bad widgets, by providing it with one more possible hypothesis, which notes that possibility and therefore, in effect, puts the robot on the lookout for it. As before, let proposition A mean that we have a box with 1/3 defective, and proposition B is the statement that we have a box with 1/6 bad. We add a third proposition, C, that something went entirely wrong with the machine that made our widgets, and it is turning out 99% defective.

Now we have to adjust our prior probabilities to take this new possibility into account. But we do not want this to be a major change in the nature of the problem; so let hypothesis C have a very low prior probability P(C|X) of $$10^{−6}$$ (−60 db). We could write out X as a verbal statement which would imply this, but as in the previous footnote we can state what proposition X is, with no ambiguity at all for the robot’s purposes, simply by giving it the probabilities conditional on X, of all the propositions that we’re going to use in this problem. In that way we don’t state everything about X that is important to us conceptually; but we state everything about X that is relevant to the robot’s current mathematical problem.

So, suppose we start out with these initial probabilities:

 $$P(A|X) = \frac {1}{11} (1 − 10^{−6}),$$
 $$P(B|X) = \frac {10}{11} (1 − 10^{−6}),$$ (4.31)
 $$P(C|X) = 10^{−6},$$

where

 A ≡ we have a box with 1/3 defective,
 B ≡ we have a box with 1/6 defective,
 C ≡ we have a box with 99/100 defective.

The factors $$(1 − 10^{−6})$$ are practically negligible, and for all practical purposes we will start out with the initial values of evidence:

 −10 db for A,
 +10 db for B,
 −60 db for C.       (4.32)

The data proposition D stands for the statement that ‘m widgets were tested and every one was defective’. Now, from (4.9), the posterior evidence for proposition C is equal to the prior evidence plus ten times the logarithm of this probability ratio:

 $$e(C|DX) = e(C|X) + 10 \log_{10} \frac {P(D|CX)}{P(D|\bar{C}X)}.$$ (4.33)

Our discussion of sampling with and without replacement in Chapter 3 shows that

 $$P(D|CX) = ( \frac {99}{100} )^m$$   (4.34)

is the probability that the first m are all bad, given that 99% of the machine’s output is bad, under our assumption that the total number in the box is large compared with the number m tested.

We also need the probability $$P(D|\bar{C}X)$$, which we can evaluate by two applications of the product rule (4.3):

 $$P(D|\bar{C}X) = P(D|X) \frac {P(\bar{C}|DX)} {P(\bar{C}|X)}.$$ (4.35)

In this problem, the prior information states dogmatically that there are only three possibilities, and so the statement $$\bar{C} ≡ ‘C$$ is false’ implies that either A or B must be true:

 $$P(\bar{C}|DX) = P(A + B|DX) = P(A|DX) + P(B|DX),$$ (4.36)

where we used the general sum rule (2.66), the negative term dropping out because A and B are mutually exclusive. Similarly,

 $$P(\bar{C}|X) = P(A|X) + P(B|X).$$ (4.37)

Now, if we substitute (4.36) into (4.35), the product rule will be applicable again in the form

 P(AD|X) = P(D|X)P(A|DX) = P(A|X)P(D|AX)
 P(BD|X) = P(D|X)P(B|DX) = P(B|X)P(D|BX),   (4.38)

and so (4.35) becomes

 $$P(D|\bar{C}X) = \frac {P(D|AX)P(A|X) + P(D|BX)P(B|X)}{P(A|X) + P(B|X)},$$ (4.39)

in which all probabilities are known from the statement of the problem.

=== 4.4.1 Digression on another derivation

Although we have the desired result (4.39), let us note that there is another way of deriving it, which is often easier than direct application of (4.3). The principle was introduced in our derivation of (3.33): resolve the proposition whose probability is desired (in this case D) into mutually exclusive propositions, and calculate the sum of their probabilities. We can carry out this resolution in many different ways by ‘introducing into the conversation’ any set of mutually exclusive and exhaustive propositions {P, Q, R, . . .} and using the rules of Boolean algebra:

 D = D(P + Q + R +···) = DP + DQ + DR + ··· . (4.40)

But the success of the method depends on our cleverness at choosing a particular set for which we can complete the calculation. This means that the propositions introduced must have a known kind of relevance to the question being asked; the example of penguins at the end of Chapter 2 will not be helpful if that question has nothing to do with penguins.

In the present case, for evaluation of $$P(D|\bar{C}X)$$, it appears that propositions A and B have this kind of relevance. Again, we note that proposition $$\bar{C}$$ implies (A + B); and so

 $$P(D|\bar{C}X) = P(D(A + B)|\bar{C}X) = P(DA + DB|\bar{C}X)$$
 $$= P(DA|\bar{C}X) + P(DB|\bar{C}X).$$   (4.41)

These probabilities can be factored by the product rule:

 $$P(D|\bar{C}X) = P(D|A\bar{C}X)P(A|\bar{C}X) + P(D|B\bar{C}X)P(B|\bar{C}X).$$ (4.42)

But we can abbreviate: $$P(D|A\bar{C}X) ≡ P(D|AX)$$ and $$P(D|B\bar{C}X) ≡ P(D|BX)$$, because, in the way we set up this problem, the statement that either A or B is true implies that C must be false. For this same reason, P(\bar{C}|AX) = 1, and so, by the product rule,

 $$P(A|\bar{C}X) = \frac {P(A|X)}{P(\bar{C}|X)},$$ (4.43)

and similarly for $$P(B|\bar{C}X)$$. Substituting these results into (4.42) and using (4.37), we again arrive at (4.39). This agreement provides another illustration – and a rather severe test – of the consistency of our rules for extended logic.

Returning to (4.39), we have the numerical value

 $$P(D|\bar{C}X) = (\frac {1}{3})^m ( \frac {1}{11}) + ( \frac {1}{6} )^m \frac {10}{11},$$ (4.44)

and everything in (4.33) is now at hand. If we put all these things together, we find that the evidence for proposition C is:

 $$e(C|DX) = −60 + 10 \log_{10} [ \frac { (\frac {99}{100})^m } { \frac {1}{11} ( \frac {1}{3})^m + \frac {10}{11} ( \frac{1}{6})^m ].$$ (4.45)

If m > 5, a good approximation is

 $$e(C|DX) \simeq −49.6 + 4.73 m, m > 5, $$  (4.46)

and if m < 3, a crude approximation is

 $$e(C|DX) \simeq −60 + 7.73 m, m < 3.$$ (4.47)

Proposition C starts out at −60 db, and the first few bad widgets we find will each give about 7.73 db of evidence in favor of C, so the graph of e(C|DX) vs. m will start upward at a slope of 7.73. But then the slope drops, when m > 5, to 4.73. The evidence for C reaches 0 db when $$m \simeq 49.6/4.73 = 10.5$$. So, ten consecutive bad widgets would be enough to raise this initially very improbable hypothesis by 58 db, to the place where the robot is ready to consider it very seriously; and 11 consecutive bad ones would take it over the threshold, to where the robot considers it more likely to be true than false.

In the meantime, what is happening to our propositions A and B? As before, A starts off at −10 db, B starts off at +10 db, and the plausibility for A starts going up 3 db per defective widget. But after we’ve found too many bad ones, that skepticism would set in, and you and I would begin to doubt whether the evidence really supports proposition A after all; proposition C is becoming a much easier way to explain what is observed. Has the robot also learned to be skeptical?

After m widgets have been tested, and all proved to be bad, the evidence for propositions A and B, and the approximate forms, are as follows:

 $$e(A|DX) = −10 + 10 \log_{10} [ \frac {(\frac{1}{3})^m} { (\frac{1}{6})^m + \frac{11}{10} × 10^{−6} (\frac{99}{100})^m} ]$$
 $$ \simeq \{ − 10 + 3m for m < 7 \} $$
 $$        \{ + 49.6 − 4.73m for m > 8 \} $$

Fig. 4.1. A surprising multiple sequential test wherein a dead hypothesis (C) is resurrected.

 $$ e(B|DX) = +10 + 10 \log_{10} [ \frac {\frac{1}{6})^m} { (\frac{1}{3})^m + 11 × 10_{−6} (\frac{99}{100})^m ]$$
 $$ \simeq \{ 10 − 3m for m < 10 \}$$
 $$ \{59.6 − 7.33m for m > 11\}$$. (4.49)

The exact results are summarized in Figure 4.1. We can learn quite a lot about multiple hypothesis testing from studying this diagram. The initial straight line part of the A and B curves represents the solution as we found it before we introduced proposition C; the change in plausibility for propositions A and B starts off just the same as in the previous problem. The effect of proposition C does not appear until we have reached the place where C crosses B. At this point, suddenly the character of the A curve changes; instead of going on up, at m = 7 it has reached its highest value of 10 db. Then it turns around and comes back down; the robot has indeed learned how to become skeptical. But the B curve does not change at this point; it continues on linearly until it reaches the place where A and C have the same plausibility, and at this point it has a change in slope. From then on, it falls off more rapidly.

Most people find all this surprising and mysterious at first glance; but then a little meditation is enough to make us perceive what is happening and why. The change in plausibility for A due to one more test arises from the fact that we are now testing hypothesis A against two alternatives: B and C. But, initially, B is so much more plausible than C, that for all practical purposes we are simply testing A against B, and reproducing our previous solution (4.22). After enough evidence has accumulated to bring the plausibility for C up to the same level as B, then from that point on A is essentially being tested against C instead of B, which is a very different situation.

All of these changes in slope can be interpreted in this way. Once we see this principle, it is clear that the same thing is going to be true more generally. As long as we have a discrete set of hypotheses, a change in plausibility for any one of them will be approximately the result of a test of this hypothesis against a single alternative – the single alternative being that one of the remaining hypotheses which is most plausible at that time. As the relative plausibilities of the alternatives change, the slope of the A curve must also change; this is the cogent information that would be lost if we tried to retain the independent additive form (4.13) when n > 2.

Whenever the hypotheses are separated by about 10 db or more, then multiple hypothesis testing reduces approximately to testing each hypothesis against a single alternative. So, seeing this, you can construct curves of the sort shown in Fig. 4.1 very rapidly without even writing down the equations, because what would happen in the two-hypothesis case is easily seen once and for all. The diagram has a number of other interesting geometrical properties, suggested by drawing the six asymptotes and noting their vertical alignment (dotted lines), which we leave for the reader to explore.

All the information needed to construct fairly accurate charts resulting from any sequence of good and bad tests is contained in the ‘plausibility flow diagrams’ of Figure 4.2, which summarize the solutions of all those binary problems; every possible way to test one proposition against a single alternative. It indicates, for example, that finding a good widget raises the evidence for B by 1 db if B is being tested against A, and by 19.22 db if it is being tested against C. Similarly, finding a bad widget raises the evidence for A by 3 db if A is being tested against B, but lowers it by 4.73 db if it is being tested against C. Likewise, we see that finding a single good widget lowers the evidence for C by an amount that cannot be recovered by two bad ones; so there is a ‘threshold of skepticism’. C will never attain an appreciable probability; i.e. the robot will never become skeptical about propositions A and B, as long as the observed fraction f of bad ones remains less than 2/3.

More precisely,we define a threshold fraction ft thus: as the number of testsm →∞ with f = mb/m → const., e(C|DX) tends to +∞ if f > ft, and to −∞ if f < ft. The exact threshold turns out to be greater than 2/3: $$f_t$$ = 0.793951 (Exercise 4.2). If the observed

Fig. 4.2. Plausibility flow diagrams.

fraction of bad widgets remains above this value, the robot will be led eventually to prefer proposition C over A and B.

Exercise 4.2. Calculate the exact threshold of skepticism ft(x, y), supposing that proposition C has instead of $$10^{−6}$$ an arbitrary prior probability P(C|X) = x, and specifies instead of 99/100 an arbitrary fraction y of bad widgets. Then discuss how the dependence on x and y corresponds – or fails to correspond – to human common sense.

Hint: In problems like this, always try first to get an analytic solution in closed form. If you are unable to do this, then you must write a short computer program which will display the correct numerical values in tables or graphs.

Exercise 4.3. Show how to make the robot skeptical about both unexpectedly high and unexpectedly low numbers of bad widgets in the observed sample. Give the full equations. Note particularly the following: if A is true, then wewould expect, according to the binomial distribution (3.86), that the observed fraction of bad ones would tend to about 1/3 with many tests, while if B is true it should tend to 1/6. Suppose that it is found to tend to the threshold value (4.24), close to 1/4. On sufficiently large m, you and I would then become skeptical about A and B; but intuition tells us that this would require a much larger m than ten, which was enough to make us and the robot skeptical when we find them all bad. Do the equations agree with our intuition here, if a new hypothesis F is introduced which specifies P(bad|FX) $$\simeq$$ 1/4?

In summary, the role of our new hypothesis C was only to be held in abeyance until needed, like a fire extinguisher. In a normal testing situation it is ‘dead’, playing no part in the inference because its probability is and remains far below that of the other hypotheses. But a dead hypothesis can be resurrected to life by very unexpected data. Exercises 4.2 and 4.3 ask the reader to explore the phenomenon of resurrection of dead hypotheses in more detail than we do in this chapter, but we return to the subject in Chapter 5.

Figure 4.1 shows an interesting thing. Suppose we had decided to stop the test and accept hypothesis A if the evidence for it reached +6 db. As we see, it would overshoot that value at the sixth trial. If we stopped the testing at that point, then we would never see the rest of this curve and see that it really goes down again. If we had continued the testing beyond this point, then we would have changed our minds again.

At first glance this seems disconcerting, but notice that it is inherent in all problems of hypothesis testing. If we stop the test at any finite number of trials, then we can never be absolutely sure that we have made the right decision. It is always possible that still more tests would have led us to change our decision. But note also that probability theory as logic has automatic built-in safety devices that can protect us against unpleasant surprises. Although it is always possible that our decision is wrong, this is extremely improbable if our critical level for decision requires e(A|DX) to be large and positive. For example, if e(A|DX) ≥ 20 db, then P(A|DX) > 0.99, and the total probability for all the alternatives is less than 0.01; then few of us would hesitate to decide confidently in favor of A.

In a real problem we may not have enough data to give such good evidence, and we might suppose that we could decide safely if the most likely hypothesis A is well separated from the alternatives, even though e(A|DX) is itself not large. Indeed, if there are 1000 alternatives but the separation of A from the most likely alternative is more than 20 db, then the odds favor A by more than 100:1 over any one of the alternatives, and if we were obliged to make a definite choice of one hypothesis here and now, there could still be no hesitation in choosing A; it is clearly the best we can do with the information we have. Yet we cannot do it so confidently, for it is now very plausible that the decision is wrong, because the class of alternatives as a whole is about as probable as A. But probability theory warns us, by the numerical value of e(A|DX), that this is the case; we need not be surprised by it.

In scientific inference our job is always to do the best we can with whatever information we have; there is no advance guarantee that our information will be sufficient to lead us to the truth. But many of the supposed difficulties arise from an inexperienced user’s failure to recognize and use the safety devices that probability theory as logic always provides. Unfortunately, the current literature offers little help here because its viewpoint, concentrated mainly on sampling theory, directs attention to other things such as assumed sampling frequencies, as the following exercises illustrate.

Exercise 4.4. Suppose that B is in fact true; estimate how many tests it will probably require in order to accumulate an additional 20 db of evidence (above the prior 10 db) in favor of B. Show that the sampling probability that we could ever obtain 20 db of evidence for A is negligibly small, even if we sample millions of times. In otherwords it is, for all practical purposes, impossible for a doctrinaire zealot to sample to a foregone false conclusion merely by continuing until he finally gets the evidence he wants.

Note: The calculations called for here are called ‘random walk’ problems; they are sampling theory exercises. Of course, the results are not wrong, only incomplete. Some essential aspects of inference in the real world are not recognized by sampling theory.

Exercise 4.5. The estimate asked for in Exercise 4.4 is called the ‘average sample number’ (ASN), and the original rationale for the sequential procedure (Wald, 1947) was not our derivation from probability theory as logic, butWald’s conjecture (unproven at the time) that the sequential probability-ratio tests such as (4.19) and (4.21) minimize the ASN for a given reliability of conclusion. Discuss the validity of this conjecture; can one define the term ‘reliability of conclusion’ in such a way that the conjecture can
be proved true?

Evidently, we could extend this example in many different directions. Introducing more ‘discrete’ hypotheses would be perfectly straightforward, as we have seen. More interesting would be the introduction of a continuous range of hypotheses, such as

 $$H_f$$ ≡ the machine is putting out a fraction f bad.

Then, instead of a discrete prior probability distribution, our robot would have a continuous distribution in 0 ≤ f ≤ 1, and it would calculate the posterior probabilities for various values of f on the basis of the observed samples, from which various decisions could be made. In fact, although we have not yet given a formal discussion of continuous probability distributions, the extension is so easy that we can give it as an introduction to this example.
