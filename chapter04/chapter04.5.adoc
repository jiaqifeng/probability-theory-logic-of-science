== 4.5 Continuous probability distribution functions

Our rules for inference were derived in Chapter 2 only for the case of finite sets of discrete propositions (A, B, . . .). But this is all we ever need in practice. Suppose that f is any continuously variable real parameter of interest, then the propositions

 F' ≡ ( f ≤ q)
 F'' ≡ ( f > q)   (4.50)

are discrete, mutually exclusive, and exhaustive; so our rules will surely apply to them. Given some information Y , the probability for F' will in general depend on q, defining a function

 G(q) ≡ P(F'|Y ), (4.51)

which is evidently monotonic increasing. Then what is the probability that f lies in any specified interval (a < f ≤ b)? The answer is probably obvious intuitively, but it is worth noting that it is determined uniquely by the sum rule of probability theory, as follows. Define the propositions

 A ≡ ( f ≤ a), B ≡ ( f ≤ b), W ≡ (a < f ≤ b). (4.52)

Then a relation of Boolean algebra is B = A + W, and since A and W are mutually exclusive, the sum rule reduces to

 P(B|Y ) = P(A|Y ) + P(W|Y ). (4.53)

But P(B|Y ) = G(b), and P(A|Y ) = G(a), so we have the result

 P(a < f ≤ b|Y ) = P(W|Y ) = G(b) − G(a). (4.54)

In the present case, G(q) is continuous and differentiable, so we may write also

 $$P(a < f ≤ b|Y ) =\int_a^b df g(f),$$ (4.55)

where g( f ) = G'( f ) ≥ 0 is the derivative of G, generally called the probability distribution function, or the probability density function for f , given Y ; either reading is consistent with the abbreviation pdf which we use henceforth, following the example of Zellner (1971). Its integral G( f ) may be called the cumulative distribution function for f .

Thus, limiting our basic theory to finite sets of propositions has not in any way hindered our ability to deal with continuous probability distributions; we have applied the basic product and sum rules only to discrete propositions in finite sets. As long as continuous distributions are defined as above (Eqs. (4.54), (4.55)) from a basis of finite sets of propositions, we are protected from inconsistencies by Cox’s theorems. But if we become overconfident and try to operate directly on infinite sets without considering how they are to be generated from finite sets, this protection is lost and we stand at the mercy of all the paradoxes of infinite-set theory, as discussed in Chapter 15; we can then derive sense and nonsense with equal ease.

We must warn the reader about another semantic confusion which has caused error and controversy in probability theory for many decades. It would be quite wrong and misleading to call g( f ) the ‘posterior distribution of f ’, because that verbiage would imply to the unwary that f itself is varying and is ‘distributed’ in some way. This would be another form of the mind projection fallacy, confusing reality with a state of knowledge about reality. In the problem we are discussing, f is simply an unknown constant parameter; what is ‘distributed’ is not the parameter, but the probability. Use of the terminology ‘probability distribution for f ’ will be followed, in order to emphasize this constantly.

Of course, nothing in probability theory forbids us to consider the possibility that f might vary with time or with circumstance; indeed, probability theory enables us to analyze that case fully, as we shall see later. But then we should recognize that we are considering a different problem than the one just discussed; it involves different quantities with different states of knowledge about them, and requires a different calculation. Confusion of these two problems is perhaps the major occupational disease of those who fool themselves by using the above misleading terminology. The pragmatic consequence is that one is led to quite wrong conclusions about the accuracy and range of validity of the results.

Questions about what happens when G(q) is discontinuous at a point q0 are discussed further in Appendix B; for the present it suffices to note that, of course, approaching a discontinuous G(q) as the limit of a sequence of continuous functions leads us to the correct results. As Gauss stressed long ago, any kind of singular mathematics acquires a meaning only as a limiting form of some kind of well-behaved mathematics, and it is ambiguous until we specify exactly what limiting process we propose to use. In this sense, singular mathematics has necessarily a kind of anthropomorphic character; the question is not what is it, but rather how shall we define it so that it is in some way useful to us?

In the present case, we approach the limit in such a way that the density function develops a sharper and sharper peak, going in the limit into a delta function $$p_0 δ(q − q_0)$$ signifying a discrete hypothesis $$H_0$$, and enclosing a limiting area equal to the probability $$p_0$$ of that hypothesis; Eq. (4.65) below is an example.

But, in fact, if we become pragmatic we note that f is not really a continuously variable parameter. In its working lifetime, a machine will produce only a finite number of widgets; if it is so well built that it makes $$10^8$$ of them, then the possible values of f are a finite set of integer multiples of $$10^{−8}$$. Then our finite-set theory will apply, and consideration of a continuously variable f is only an approximation to the exact discrete theory. There is never any need to consider infinite sets or measure theory in the real, exact problem. Likewise, any data set that can actually be recorded and analyzed is digitized into multiples of some smallest element. Most cases of allegedly continuously variable quantities are like this when one takes note of the actual, real-world situation.
